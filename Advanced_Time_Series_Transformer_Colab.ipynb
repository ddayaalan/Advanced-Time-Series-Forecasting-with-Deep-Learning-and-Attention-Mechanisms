{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec24929f",
   "metadata": {},
   "source": [
    "\n",
    "# Advanced Time Series Forecasting with Deep Learning & Attention (Colab)\n",
    "\n",
    "This notebook includes:\n",
    "- Synthetic dataset generation\n",
    "- Sliding window preprocessing\n",
    "- Transformer-based forecasting model\n",
    "- Evaluation using RMSE & MAE\n",
    "- ARIMA baseline comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f705083",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!pip install torch numpy pandas scikit-learn matplotlib statsmodels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc094c9",
   "metadata": {},
   "source": [
    "## Dataset Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a052a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)\n",
    "T = 3000\n",
    "t = np.arange(T)\n",
    "\n",
    "trend = 0.005 * t\n",
    "seasonal1 = 2 * np.sin(2 * np.pi * t / 50)\n",
    "seasonal2 = 1.5 * np.sin(2 * np.pi * t / 365)\n",
    "long_dep = 0.8 * np.sin(2 * np.pi * t / 800)\n",
    "noise = np.random.normal(0, 0.5, T)\n",
    "\n",
    "series = 10 + trend + seasonal1 + seasonal2 + long_dep + noise\n",
    "df = pd.DataFrame({\"value\": series})\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2c7c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.plot(df['value'])\n",
    "plt.title(\"Synthetic Time Series\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275a450b",
   "metadata": {},
   "source": [
    "## Data Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af74a239",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "SEQ_LEN = 60\n",
    "PRED_LEN = 1\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, values):\n",
    "        self.scaler = StandardScaler()\n",
    "        self.values = self.scaler.fit_transform(values.reshape(-1,1))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.values) - SEQ_LEN - PRED_LEN\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.values[idx:idx+SEQ_LEN]\n",
    "        y = self.values[idx+SEQ_LEN:idx+SEQ_LEN+PRED_LEN]\n",
    "        return torch.FloatTensor(x), torch.FloatTensor(y)\n",
    "\n",
    "dataset = TimeSeriesDataset(df['value'].values)\n",
    "train_size = int(0.7 * len(dataset))\n",
    "val_size = int(0.15 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "train_ds, val_ds, test_ds = torch.utils.data.random_split(dataset, [train_size, val_size, test_size])\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51b449e",
   "metadata": {},
   "source": [
    "## Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d1e96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class TransformerTS(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Linear(1, 64)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=64, nhead=4, batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=2)\n",
    "        self.fc = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "        x = self.transformer(x)\n",
    "        return self.fc(x[:, -1, :])\n",
    "\n",
    "model = TransformerTS()\n",
    "model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff61619",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c16e381",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()\n",
    "EPOCHS = 15\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for x, y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(x)\n",
    "        loss = criterion(pred, y.squeeze())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {total_loss/len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e565faf",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1943d060",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import numpy as np\n",
    "\n",
    "model.eval()\n",
    "y_true, y_pred = [], []\n",
    "\n",
    "for x, y in DataLoader(test_ds, batch_size=64):\n",
    "    with torch.no_grad():\n",
    "        pred = model(x)\n",
    "    y_true.extend(y.numpy())\n",
    "    y_pred.extend(pred.numpy())\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "mae = mean_absolute_error(y_true, y_pred)\n",
    "rmse, mae\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce7431c",
   "metadata": {},
   "source": [
    "## ARIMA Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6179cd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "train_series = df['value'][:2000]\n",
    "test_series = df['value'][2000:]\n",
    "\n",
    "model_arima = ARIMA(train_series, order=(5,1,0))\n",
    "fit = model_arima.fit()\n",
    "forecast = fit.forecast(len(test_series))\n",
    "\n",
    "rmse_arima = np.sqrt(mean_squared_error(test_series, forecast))\n",
    "rmse_arima\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
